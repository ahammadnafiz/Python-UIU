{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headers to mimic a browser visit\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "    'Accept-Language': 'en-US,en;q=0.5',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "}\n",
    "\n",
    "def get_property_links(page_url):\n",
    "    \"\"\"Extract all property links from a page\"\"\"\n",
    "    try:\n",
    "        response = requests.get(page_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find all property links\n",
    "        property_links = []\n",
    "        link_elements = soup.select('a.js-listing-link')\n",
    "        \n",
    "        for link in link_elements:\n",
    "            href = link.get('href')\n",
    "            if href and 'bproperty.com' in href:\n",
    "                property_links.append(href)\n",
    "        \n",
    "        return property_links\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching page {page_url}: {e}\")\n",
    "        return []\n",
    "\n",
    "def get_next_page_url(page_url):\n",
    "    \"\"\"Extract the next page URL if available\"\"\"\n",
    "    try:\n",
    "        response = requests.get(page_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        next_page_div = soup.select_one('div.next')\n",
    "        if next_page_div:\n",
    "            next_page_a = next_page_div.find('a')\n",
    "            if next_page_a:\n",
    "                return next_page_a.get('href')\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding next page from {page_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text by removing extra whitespace and newlines\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def extract_price(price_text):\n",
    "    \"\"\"Extract numeric price from text\"\"\"\n",
    "    if not price_text:\n",
    "        return \"\"\n",
    "    # Extract the price value (৳X,XXX,XXX)\n",
    "    match = re.search(r'৳([\\d,]+)', price_text)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return price_text.strip()\n",
    "\n",
    "def scrape_property_details(property_url):\n",
    "    \"\"\"Scrape details from a property page\"\"\"\n",
    "    try:\n",
    "        response = requests.get(property_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Extract title\n",
    "        title_element = soup.select_one('h1.Title-pdp-title span')\n",
    "        title = clean_text(title_element.text) if title_element else \"\"\n",
    "        \n",
    "        # Extract location\n",
    "        location_element = soup.select_one('h3.Title-pdp-address')\n",
    "        location = \"\"\n",
    "        if location_element:\n",
    "            # Remove the location icon span\n",
    "            icon_span = location_element.select_one('span.icon-pin')\n",
    "            if icon_span:\n",
    "                icon_span.extract()\n",
    "            location = clean_text(location_element.text)\n",
    "        \n",
    "        # Extract price\n",
    "        price_element = soup.select_one('div.Title-pdp-price span.FirstPrice')\n",
    "        price = extract_price(price_element.text) if price_element else \"\"\n",
    "        \n",
    "        # Extract details\n",
    "        details = {}\n",
    "        detail_elements = soup.select('div.listing-details div.columns-2')\n",
    "        for detail in detail_elements:\n",
    "            label_element = detail.select_one('div.listing-details-label')\n",
    "            value_element = detail.select_one('div.last')\n",
    "            \n",
    "            if label_element and value_element:\n",
    "                # Extract label name and clean it\n",
    "                label_text = label_element.get_text(strip=True)\n",
    "                label = clean_text(label_text).lower().replace(' ', '_')\n",
    "                \n",
    "                # Clean up label - remove icon reference\n",
    "                label = re.sub(r'^[^a-z]*', '', label)\n",
    "                \n",
    "                # Extract value\n",
    "                value = clean_text(value_element.text)\n",
    "                details[label] = value\n",
    "        \n",
    "        # Extract amenities\n",
    "        amenities = []\n",
    "        amenity_elements = soup.select('div.listing-amenities-list-item span.listing-amenities-name')\n",
    "        for amenity in amenity_elements:\n",
    "            amenities.append(clean_text(amenity.text))\n",
    "        \n",
    "        # Combine all data\n",
    "        property_data = {\n",
    "            'title': title,\n",
    "            'location': location,\n",
    "            'price': price,\n",
    "            'url': property_url,\n",
    "            'amenities': ', '.join(amenities)\n",
    "        }\n",
    "        \n",
    "        # Add details to property data\n",
    "        property_data.update(details)\n",
    "        \n",
    "        return property_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping property {property_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    base_url = \"https://www.bproperty.com/buy/dhaka/residential/apartments/\"\n",
    "    csv_file = \"bproperty_apartments_dhaka.csv\"\n",
    "    \n",
    "    # Get all property links from multiple pages\n",
    "    all_property_links = []\n",
    "    current_page_url = base_url\n",
    "    page_count = 1\n",
    "    \n",
    "    while current_page_url and page_count <= 50:\n",
    "        print(f\"Fetching page {page_count}: {current_page_url}\")\n",
    "        property_links = get_property_links(current_page_url)\n",
    "        all_property_links.extend(property_links)\n",
    "        print(f\"Found {len(property_links)} properties on page {page_count}\")\n",
    "        \n",
    "        # Get next page URL\n",
    "        current_page_url = get_next_page_url(current_page_url)\n",
    "        page_count += 1\n",
    "        \n",
    "        # Random delay to avoid rate limiting\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "    \n",
    "    print(f\"Total properties found: {len(all_property_links)}\")\n",
    "    \n",
    "    # Scrape details for each property\n",
    "    all_property_data = []\n",
    "    total_properties = len(all_property_links)\n",
    "    \n",
    "    for i, property_url in enumerate(all_property_links, 1):\n",
    "        print(f\"Scraping property {i}/{total_properties}: {property_url}\")\n",
    "        property_data = scrape_property_details(property_url)\n",
    "        \n",
    "        if property_data:\n",
    "            all_property_data.append(property_data)\n",
    "        \n",
    "        # Random delay to avoid rate limiting\n",
    "        time.sleep(random.uniform(2, 5))\n",
    "    \n",
    "    # Find all unique fields across all properties\n",
    "    all_fields = set()\n",
    "    for property_data in all_property_data:\n",
    "        all_fields.update(property_data.keys())\n",
    "    \n",
    "    # Sort fields to ensure consistent column order\n",
    "    field_names = sorted(list(all_fields))\n",
    "    \n",
    "    # Move key fields to the beginning\n",
    "    key_fields = ['title', 'location', 'price', 'url', 'bedrooms', 'bathrooms', 'floor_area', 'date', 'builtin_year', 'occupancy_status', 'amenities']\n",
    "    for field in reversed(key_fields):\n",
    "        if field in field_names:\n",
    "            field_names.remove(field)\n",
    "            field_names.insert(0, field)\n",
    "    \n",
    "    # Write data to CSV\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=field_names)\n",
    "        writer.writeheader()\n",
    "        for property_data in all_property_data:\n",
    "            writer.writerow({field: property_data.get(field, '') for field in field_names})\n",
    "    \n",
    "    print(f\"Successfully scraped {len(all_property_data)} properties\")\n",
    "    print(f\"Data saved to {csv_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
