{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "\n",
    "def get_product_links(base_url, pages=25):\n",
    "    \"\"\"Extract all laptop product links from multiple pages\"\"\"\n",
    "    all_links = []\n",
    "    \n",
    "    for page in range(1, pages + 1):\n",
    "        if page == 1:\n",
    "            url = f\"{base_url}/laptop-notebook/laptop\"\n",
    "        else:\n",
    "            url = f\"{base_url}/laptop-notebook/laptop?page={page}\"\n",
    "        \n",
    "        print(f\"Fetching links from page {page}...\")\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            product_items = soup.find_all('h4', class_='p-item-name')\n",
    "            \n",
    "            for item in product_items:\n",
    "                link = item.find('a')['href']\n",
    "                all_links.append(link)\n",
    "        else:\n",
    "            print(f\"Failed to fetch page {page}. Status code: {response.status_code}\")\n",
    "        \n",
    "        # Add delay to avoid overwhelming the server\n",
    "        time.sleep(1)\n",
    "    \n",
    "    print(f\"Total {len(all_links)} product links found.\")\n",
    "    return all_links\n",
    "\n",
    "def extract_specification_data(soup):\n",
    "    \"\"\"Extract all specification data from the product page\"\"\"\n",
    "    spec_dict = {}\n",
    "    \n",
    "    # Get specification table\n",
    "    spec_tables = soup.find_all('table', class_='data-table')\n",
    "    \n",
    "    if not spec_tables:\n",
    "        return spec_dict\n",
    "    \n",
    "    for table in spec_tables:\n",
    "        # Get all section headers\n",
    "        headers = table.find_all('td', class_='heading-row')\n",
    "        \n",
    "        for header in headers:\n",
    "            section_name = header.text.strip()\n",
    "            # Find the next tbody after this header\n",
    "            next_tbody = header.parent.parent.find_next('tbody')\n",
    "            \n",
    "            # Extract all rows from this section\n",
    "            if next_tbody:\n",
    "                rows = next_tbody.find_all('tr')\n",
    "                for row in rows:\n",
    "                    name_cell = row.find('td', class_='name')\n",
    "                    value_cell = row.find('td', class_='value')\n",
    "                    if name_cell and value_cell:\n",
    "                        key = f\"{section_name}_{name_cell.text.strip()}\"\n",
    "                        value = value_cell.text.strip().replace('\\n', ' ')\n",
    "                        spec_dict[key] = value\n",
    "    \n",
    "    return spec_dict\n",
    "\n",
    "def extract_product_info(soup):\n",
    "    \"\"\"Extract basic product information\"\"\"\n",
    "    info_dict = {}\n",
    "    \n",
    "    # Get product name\n",
    "    product_name_element = soup.find('h1', class_='product-name')\n",
    "    if product_name_element:\n",
    "        info_dict['product_name'] = product_name_element.text.strip()\n",
    "    \n",
    "    # Get product info table\n",
    "    product_info_table = soup.find('table', class_='product-info-table')\n",
    "    if product_info_table:\n",
    "        rows = product_info_table.find_all('tr', class_='product-info-group')\n",
    "        for row in rows:\n",
    "            label = row.find('td', class_='product-info-label')\n",
    "            data = row.find('td', class_='product-info-data')\n",
    "            if label and data:\n",
    "                key = label.text.strip()\n",
    "                value = data.text.strip()\n",
    "                # Clean the price value by removing currency symbol\n",
    "                if key == 'Price' or key == 'Regular Price':\n",
    "                    value = re.sub(r'[^\\d.]', '', value)\n",
    "                info_dict[key] = value\n",
    "    \n",
    "    return info_dict\n",
    "\n",
    "def scrape_product_details(url):\n",
    "    \"\"\"Scrape all details from a product page\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch {url}. Status code: {response.status_code}\")\n",
    "            return {}\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extract product info and specifications\n",
    "        product_info = extract_product_info(soup)\n",
    "        specifications = extract_specification_data(soup)\n",
    "        \n",
    "        # Combine all data\n",
    "        combined_data = {**product_info, **specifications, 'product_url': url}\n",
    "        \n",
    "        return combined_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {str(e)}\")\n",
    "        return {}\n",
    "\n",
    "def main():\n",
    "    base_url = \"https://www.startech.com.bd\"\n",
    "    \n",
    "    # Get all product links\n",
    "    all_product_links = get_product_links(base_url)\n",
    "    \n",
    "    # Scrape details from each product page\n",
    "    all_products_data = []\n",
    "    \n",
    "    for i, link in enumerate(all_product_links):\n",
    "        print(f\"Scraping product {i+1} of {len(all_product_links)}: {link}\")\n",
    "        product_data = scrape_product_details(link)\n",
    "        if product_data:\n",
    "            all_products_data.append(product_data)\n",
    "        \n",
    "        # Add delay to avoid overwhelming the server\n",
    "        time.sleep(2)\n",
    "    \n",
    "    # Create DataFrame and save to CSV\n",
    "    if all_products_data:\n",
    "        df = pd.DataFrame(all_products_data)\n",
    "        \n",
    "        # Save to CSV\n",
    "        csv_filename = 'startech_laptops_data.csv'\n",
    "        df.to_csv(csv_filename, index=False)\n",
    "        print(f\"Data saved to {csv_filename}. Total items: {len(df)}\")\n",
    "    else:\n",
    "        print(\"No data was collected.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
