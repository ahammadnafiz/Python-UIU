{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# User agent to mimic a browser\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.8',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "    'Cache-Control': 'max-age=0'\n",
    "}\n",
    "\n",
    "def get_amazon_page(url):\n",
    "    \"\"\"Get Amazon page HTML with retry logic\"\"\"\n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                return response.text\n",
    "            else:\n",
    "                print(f\"Error {response.status_code}. Retrying ({attempt+1}/{max_retries})...\")\n",
    "                time.sleep(5 + random.random() * 5)  # Random delay between 5-10 seconds\n",
    "        except Exception as e:\n",
    "            print(f\"Exception: {e}. Retrying ({attempt+1}/{max_retries})...\")\n",
    "            time.sleep(5 + random.random() * 5)\n",
    "    \n",
    "    print(f\"Failed to retrieve page: {url}\")\n",
    "    return None\n",
    "\n",
    "def extract_product_links(html_content):\n",
    "    \"\"\"Extract product links from Amazon search results page\"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    product_links = []\n",
    "    \n",
    "    # Look for product links using the structure from your example\n",
    "    product_elements = soup.select('a.a-link-normal.s-faceout-link.aok-block.a-text-normal')\n",
    "    \n",
    "    # If the above selector doesn't work, try alternative selectors\n",
    "    if not product_elements:\n",
    "        product_elements = soup.select('a.a-link-normal[href*=\"/dp/\"]')\n",
    "    \n",
    "    for element in product_elements:\n",
    "        href = element.get('href', '')\n",
    "        if href and '/dp/' in href:\n",
    "            # Extract just the product ID portion if it's a relative URL\n",
    "            if href.startswith('/'):\n",
    "                product_links.append(f\"https://www.amazon.com{href}\")\n",
    "            else:\n",
    "                product_links.append(href)\n",
    "    \n",
    "    return product_links\n",
    "\n",
    "def get_next_page_url(html_content, base_url=\"https://www.amazon.com\"):\n",
    "    \"\"\"Extract the next page URL from the current page\"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Look for the \"Next\" button based on your example\n",
    "    next_page_element = soup.select_one('a.s-pagination-item.s-pagination-next')\n",
    "    \n",
    "    if next_page_element and next_page_element.get('href'):\n",
    "        next_url = next_page_element.get('href')\n",
    "        if next_url.startswith('/'):\n",
    "            return f\"{base_url}{next_url}\"\n",
    "        return next_url\n",
    "    \n",
    "    return None\n",
    "\n",
    "def save_links_to_csv(df, filename=\"amazon_laptop_links.csv\"):\n",
    "    \"\"\"Save the collected links DataFrame to a CSV file\"\"\"\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Saved {len(df)} links to {filename}\")\n",
    "\n",
    "def scrape_amazon_laptops(start_url, max_pages=100):\n",
    "    \"\"\"Main function to scrape laptop links from Amazon\"\"\"\n",
    "    current_url = start_url\n",
    "    all_product_links = []\n",
    "    page_count = 0\n",
    "    \n",
    "    while current_url and page_count < max_pages:\n",
    "        page_count += 1\n",
    "        print(f\"Scraping page {page_count}: {current_url}\")\n",
    "        \n",
    "        # Get the page HTML\n",
    "        html_content = get_amazon_page(current_url)\n",
    "        if not html_content:\n",
    "            print(f\"Failed to get page {page_count}. Stopping.\")\n",
    "            break\n",
    "        \n",
    "        # Extract product links\n",
    "        product_links = extract_product_links(html_content)\n",
    "        print(f\"Found {len(product_links)} product links on page {page_count}\")\n",
    "        \n",
    "        # Add page number information\n",
    "        page_data = [(link, page_count) for link in product_links]\n",
    "        all_product_links.extend(page_data)\n",
    "        \n",
    "        # Create DataFrame and save progress every 5 pages\n",
    "        if page_count % 5 == 0:\n",
    "            df = pd.DataFrame(all_product_links, columns=['product_url', 'page_number'])\n",
    "            save_links_to_csv(df, f\"amazon_laptop_links_progress_{page_count}.csv\")\n",
    "        \n",
    "        # Get the next page URL\n",
    "        current_url = get_next_page_url(html_content)\n",
    "        \n",
    "        # Add a random delay to avoid being blocked\n",
    "        delay = 3 + random.random() * 7  # Random delay between 3-10 seconds\n",
    "        print(f\"Waiting {delay:.2f} seconds before next request...\")\n",
    "        time.sleep(delay)\n",
    "    \n",
    "    # Create final DataFrame and save all collected links\n",
    "    df_final = pd.DataFrame(all_product_links, columns=['product_url', 'page_number'])\n",
    "    \n",
    "    # Extract product IDs from URLs\n",
    "    df_final['product_id'] = df_final['product_url'].apply(\n",
    "        lambda url: re.search(r'/dp/([A-Z0-9]{10})', url).group(1) if re.search(r'/dp/([A-Z0-9]{10})', url) else None\n",
    "    )\n",
    "    \n",
    "    # Save the DataFrame\n",
    "    save_links_to_csv(df_final)\n",
    "    \n",
    "    print(f\"Finished scraping {page_count} pages. Total links collected: {len(df_final)}\")\n",
    "    return df_final\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Use the base URL for Amazon laptop search\n",
    "    start_url = \"https://www.amazon.com/s?k=laptops\"\n",
    "    \n",
    "    # Start scraping\n",
    "    df_products = scrape_amazon_laptops(start_url, max_pages=100)\n",
    "    \n",
    "    # Display first few rows of the data\n",
    "    print(\"\\nFirst 5 rows of collected data:\")\n",
    "    print(df_products.head())\n",
    "    \n",
    "    # Quick stats\n",
    "    print(\"\\nData Statistics:\")\n",
    "    print(f\"Total products: {len(df_products)}\")\n",
    "    print(f\"Products per page (average): {len(df_products) / df_products['page_number'].max():.2f}\")\n",
    "    print(f\"Number of pages scraped: {df_products['page_number'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User agent to mimic a browser\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.8',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "    'Cache-Control': 'max-age=0'\n",
    "}\n",
    "\n",
    "def get_amazon_page(url):\n",
    "    \"\"\"Get Amazon page HTML with retry logic\"\"\"\n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                return response.text\n",
    "            else:\n",
    "                print(f\"Error {response.status_code}. Retrying ({attempt+1}/{max_retries})...\")\n",
    "                time.sleep(5 + random.random() * 5)  # Random delay between 5-10 seconds\n",
    "        except Exception as e:\n",
    "            print(f\"Exception: {e}. Retrying ({attempt+1}/{max_retries})...\")\n",
    "            time.sleep(5 + random.random() * 5)\n",
    "    \n",
    "    print(f\"Failed to retrieve page: {url}\")\n",
    "    return None\n",
    "\n",
    "def extract_product_links(html_content):\n",
    "    \"\"\"Extract product links from Amazon search results page\"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    product_links = []\n",
    "    \n",
    "    # Look for product links using the structure from your example\n",
    "    product_elements = soup.select('a.a-link-normal.s-faceout-link.aok-block.a-text-normal')\n",
    "    \n",
    "    # If the above selector doesn't work, try alternative selectors\n",
    "    if not product_elements:\n",
    "        product_elements = soup.select('a.a-link-normal[href*=\"/dp/\"]')\n",
    "    \n",
    "    for element in product_elements:\n",
    "        href = element.get('href', '')\n",
    "        if href and '/dp/' in href:\n",
    "            # Extract just the product ID portion if it's a relative URL\n",
    "            if href.startswith('/'):\n",
    "                product_links.append(f\"https://www.amazon.com{href}\")\n",
    "            else:\n",
    "                product_links.append(href)\n",
    "    \n",
    "    return product_links\n",
    "\n",
    "def get_next_page_url(html_content, base_url=\"https://www.amazon.com\"):\n",
    "    \"\"\"Extract the next page URL from the current page\"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Look for the \"Next\" button based on your example\n",
    "    next_page_element = soup.select_one('a.s-pagination-item.s-pagination-next')\n",
    "    \n",
    "    if next_page_element and next_page_element.get('href'):\n",
    "        next_url = next_page_element.get('href')\n",
    "        if next_url.startswith('/'):\n",
    "            return f\"{base_url}{next_url}\"\n",
    "        return next_url\n",
    "    \n",
    "    return None\n",
    "\n",
    "def extract_product_details(html_content):\n",
    "    \"\"\"Extract detailed product information from a product page\"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    details = {}\n",
    "    \n",
    "    # Extract title\n",
    "    try:\n",
    "        title_element = soup.select_one('#productTitle')\n",
    "        if title_element:\n",
    "            details['title'] = title_element.text.strip()\n",
    "        else:\n",
    "            details['title'] = None\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting title: {e}\")\n",
    "        details['title'] = None\n",
    "    \n",
    "    # Extract price\n",
    "    try:\n",
    "        price_whole = soup.select_one('span.a-price-whole')\n",
    "        price_fraction = soup.select_one('span.a-price-fraction')\n",
    "        \n",
    "        if price_whole and price_fraction:\n",
    "            price = price_whole.text.strip() + price_fraction.text.strip()\n",
    "            # Remove any non-numeric characters except decimal point\n",
    "            price = re.sub(r'[^\\d.]', '', price)\n",
    "            details['price'] = float(price) if price else None\n",
    "        else:\n",
    "            details['price'] = None\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting price: {e}\")\n",
    "        details['price'] = None\n",
    "    \n",
    "    # Extract tech specs table\n",
    "    tech_specs = {}\n",
    "    try:\n",
    "        tech_table = soup.select_one('#productDetails_techSpec_section_1')\n",
    "        if tech_table:\n",
    "            rows = tech_table.select('tr')\n",
    "            for row in rows:\n",
    "                header = row.select_one('th')\n",
    "                value = row.select_one('td')\n",
    "                if header and value:\n",
    "                    key = header.text.strip()\n",
    "                    val = value.text.strip().replace('‎', '')\n",
    "                    tech_specs[key] = val\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting tech specs: {e}\")\n",
    "    \n",
    "    # Extract additional details table\n",
    "    additional_details = {}\n",
    "    try:\n",
    "        details_table = soup.select_one('#productDetails_techSpec_section_2')\n",
    "        if details_table:\n",
    "            rows = details_table.select('tr')\n",
    "            for row in rows:\n",
    "                header = row.select_one('th')\n",
    "                value = row.select_one('td')\n",
    "                if header and value:\n",
    "                    key = header.text.strip()\n",
    "                    val = value.text.strip().replace('‎', '')\n",
    "                    additional_details[key] = val\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting additional details: {e}\")\n",
    "    \n",
    "    # Add all extracted info to the details dictionary\n",
    "    details['tech_specs'] = tech_specs\n",
    "    details['additional_details'] = additional_details\n",
    "    \n",
    "    return details\n",
    "\n",
    "def save_to_csv(df, filename=\"amazon_laptops_data.csv\"):\n",
    "    \"\"\"Save the DataFrame to CSV\"\"\"\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Saved {len(df)} products to {filename}\")\n",
    "\n",
    "def flatten_dict(d, prefix=''):\n",
    "    \"\"\"Flatten nested dictionaries for DataFrame creation\"\"\"\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = f\"{prefix}_{k}\" if prefix else k\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(flatten_dict(v, new_key).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "def scrape_laptop_links(start_url, max_pages=100):\n",
    "    \"\"\"Scrape product links from search results pages\"\"\"\n",
    "    current_url = start_url\n",
    "    all_product_links = []\n",
    "    page_count = 0\n",
    "    \n",
    "    while current_url and page_count < max_pages:\n",
    "        page_count += 1\n",
    "        print(f\"Scraping search page {page_count}: {current_url}\")\n",
    "        \n",
    "        # Get the page HTML\n",
    "        html_content = get_amazon_page(current_url)\n",
    "        if not html_content:\n",
    "            print(f\"Failed to get page {page_count}. Stopping.\")\n",
    "            break\n",
    "        \n",
    "        # Extract product links\n",
    "        product_links = extract_product_links(html_content)\n",
    "        print(f\"Found {len(product_links)} product links on page {page_count}\")\n",
    "        \n",
    "        # Add page number information\n",
    "        page_data = [(link, page_count) for link in product_links]\n",
    "        all_product_links.extend(page_data)\n",
    "        \n",
    "        # Create DataFrame and save progress every 5 pages\n",
    "        if page_count % 5 == 0:\n",
    "            df = pd.DataFrame(all_product_links, columns=['product_url', 'page_number'])\n",
    "            df.to_csv(f\"amazon_laptop_links_progress_{page_count}.csv\", index=False)\n",
    "            print(f\"Saved progress: {len(df)} links collected so far\")\n",
    "        \n",
    "        # Get the next page URL\n",
    "        current_url = get_next_page_url(html_content)\n",
    "        \n",
    "        # Add a random delay to avoid being blocked\n",
    "        delay = 3 + random.random() * 7  # Random delay between 3-10 seconds\n",
    "        print(f\"Waiting {delay:.2f} seconds before next request...\")\n",
    "        time.sleep(delay)\n",
    "    \n",
    "    # Create final DataFrame with all collected links\n",
    "    df_links = pd.DataFrame(all_product_links, columns=['product_url', 'page_number'])\n",
    "    \n",
    "    # Extract product IDs from URLs\n",
    "    df_links['product_id'] = df_links['product_url'].apply(\n",
    "        lambda url: re.search(r'/dp/([A-Z0-9]{10})', url).group(1) if re.search(r'/dp/([A-Z0-9]{10})', url) else None\n",
    "    )\n",
    "    \n",
    "    print(f\"Finished scraping {page_count} pages. Total links collected: {len(df_links)}\")\n",
    "    return df_links\n",
    "\n",
    "def scrape_product_details(df_links, max_products=None):\n",
    "    \"\"\"Scrape detailed information from each product page\"\"\"\n",
    "    all_product_data = []\n",
    "    \n",
    "    # Limit number of products if specified\n",
    "    if max_products:\n",
    "        products_to_scrape = df_links.head(max_products)\n",
    "    else:\n",
    "        products_to_scrape = df_links\n",
    "    \n",
    "    total = len(products_to_scrape)\n",
    "    \n",
    "    for idx, (_, row) in enumerate(products_to_scrape.iterrows()):\n",
    "        url = row['product_url']\n",
    "        product_id = row['product_id']\n",
    "        page_number = row['page_number']\n",
    "        \n",
    "        print(f\"Scraping product {idx+1}/{total}: {url}\")\n",
    "        \n",
    "        # Get product page HTML\n",
    "        html_content = get_amazon_page(url)\n",
    "        if not html_content:\n",
    "            print(f\"Failed to get product page. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Extract product details\n",
    "        details = extract_product_details(html_content)\n",
    "        \n",
    "        # Add product ID and URL to details\n",
    "        details['product_id'] = product_id\n",
    "        details['product_url'] = url\n",
    "        details['search_page_number'] = page_number\n",
    "        \n",
    "        # Add to data list\n",
    "        all_product_data.append(details)\n",
    "        \n",
    "        # Save progress every 10 products\n",
    "        if (idx + 1) % 10 == 0:\n",
    "            # Create temporary DataFrame with current data\n",
    "            temp_df = pd.json_normalize(all_product_data)\n",
    "            temp_df.to_csv(f\"amazon_laptop_details_progress_{idx+1}.csv\", index=False)\n",
    "            print(f\"Saved progress: {len(temp_df)} products scraped\")\n",
    "        \n",
    "        # Add a random delay to avoid being blocked\n",
    "        delay = 5 + random.random() * 10  # Random delay between 5-15 seconds\n",
    "        print(f\"Waiting {delay:.2f} seconds before next request...\")\n",
    "        time.sleep(delay)\n",
    "    \n",
    "    # Process all collected data into a DataFrame\n",
    "    print(\"Processing collected data...\")\n",
    "    \n",
    "    # Create DataFrame from collected data\n",
    "    df_details = pd.DataFrame(all_product_data)\n",
    "    \n",
    "    # Extract key technical specifications into separate columns\n",
    "    key_specs = [\n",
    "        'Standing screen display size', 'Screen Resolution', 'Processor', \n",
    "        'RAM', 'Hard Drive', 'Graphics Coprocessor', 'Operating System',\n",
    "        'Average Battery Life (in hours)'\n",
    "    ]\n",
    "    \n",
    "    # Create normalized DataFrame\n",
    "    flattened_data = []\n",
    "    for product in all_product_data:\n",
    "        # Basic product info\n",
    "        flat_product = {\n",
    "            'product_id': product['product_id'],\n",
    "            'product_url': product['product_url'],\n",
    "            'search_page_number': product['search_page_number'],\n",
    "            'title': product['title'],\n",
    "            'price': product['price']\n",
    "        }\n",
    "        \n",
    "        # Extract key specs\n",
    "        tech_specs = product.get('tech_specs', {})\n",
    "        additional_details = product.get('additional_details', {})\n",
    "        \n",
    "        # Add key specs as direct columns\n",
    "        for spec in key_specs:\n",
    "            if spec in tech_specs:\n",
    "                flat_product[spec.replace(' ', '_').lower()] = tech_specs[spec]\n",
    "            elif spec in additional_details:\n",
    "                flat_product[spec.replace(' ', '_').lower()] = additional_details[spec]\n",
    "        \n",
    "        # Add some additional important details\n",
    "        if 'Brand' in additional_details:\n",
    "            flat_product['brand'] = additional_details['Brand']\n",
    "        if 'Series' in additional_details:\n",
    "            flat_product['series'] = additional_details['Series']\n",
    "        if 'Color' in additional_details:\n",
    "            flat_product['color'] = additional_details['Color']\n",
    "        if 'Item Weight' in additional_details:\n",
    "            flat_product['weight'] = additional_details['Item Weight']\n",
    "        \n",
    "        flattened_data.append(flat_product)\n",
    "    \n",
    "    # Create final DataFrame\n",
    "    df_final = pd.DataFrame(flattened_data)\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the scraper\"\"\"\n",
    "    # Use the base URL for Amazon laptop search\n",
    "    start_url = \"https://www.amazon.com/s?k=laptops\"\n",
    "    \n",
    "    # Step 1: Scrape product links\n",
    "    print(\"Step 1: Scraping product links from search results...\")\n",
    "    df_links = scrape_laptop_links(start_url, max_pages=20)\n",
    "    df_links.to_csv(\"amazon_laptop_links.csv\", index=False)\n",
    "    \n",
    "    # Step 2: Scrape product details\n",
    "    print(\"\\nStep 2: Scraping detailed information for each product...\")\n",
    "    df_details = scrape_product_details(df_links)\n",
    "    \n",
    "    # Save the final results\n",
    "    df_details.to_csv(\"amazon_laptops_complete_data.csv\", index=False)\n",
    "    \n",
    "    print(\"\\nScraping completed!\")\n",
    "    print(f\"Total products with complete details: {len(df_details)}\")\n",
    "    print(f\"Data saved to 'amazon_laptops_complete_data.csv'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
